{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO4CnpkCoRSRFUvk4RhdByR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gautam822/ICG_SUPPLY_CHAIN/blob/main/Gautam_Ginodiya_Assignment2_DS_ICG_Supply_Chain_Unveiled.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing Libraries"
      ],
      "metadata": {
        "id": "9tNug3AAxJ3K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split,KFold\n",
        "from sklearn.preprocessing import StandardScaler\n"
      ],
      "metadata": {
        "id": "QwRuT_nfxK4A"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the same dataset for all the algorithms of task 1"
      ],
      "metadata": {
        "id": "-cd0GXYP2d4_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = {\n",
        "    'x': [1, 2, 3, 4, 5],\n",
        "    'y': [2, 4, 5, 4, 5]\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "X = df['x']\n",
        "y = df['y']\n",
        "n = len(X)"
      ],
      "metadata": {
        "id": "-twxhuGn2dJF"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 1"
      ],
      "metadata": {
        "id": "0DuYDW5TyC5Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.Linear regression:-"
      ],
      "metadata": {
        "id": "V88Wg0nEydMR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cost Function(Mean squared error):"
      ],
      "metadata": {
        "id": "SzA2JjiiyioC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$\n",
        "\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n",
        "$$"
      ],
      "metadata": {
        "id": "WvcUVRVSyi2S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The best fit line is y=m*x+c\n",
        "\n",
        "*  n \\: number of data points  \n",
        "*  yi \\: actual value (ground truth)  \n",
        "*  yi(hat) : predicted value by the model"
      ],
      "metadata": {
        "id": "atRt0HOb05Js"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ordinary Least Squares"
      ],
      "metadata": {
        "id": "ujloxneB2N-b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# If we do the calculation of differentiating and putting it equal to zero then we get the mathematical relation for m and c as follows:\n",
        "m=(n*(X*y).sum()-(X.sum()*y.sum()))/((n*(X**2).sum())-(X.sum()**2))\n",
        "c=(y.sum()-m*X.sum())/n\n",
        "df['y_pred'] = m * X + c\n",
        "mse_ols = ((df['y'] - df['y_pred'])**2).mean()"
      ],
      "metadata": {
        "id": "nsFC5cvryg7q"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gradient Descent\n",
        "\n",
        "Usefull terms:  Learning rate= Step size\n",
        "\n",
        "Epochs: Iterations."
      ],
      "metadata": {
        "id": "1hlMMTHm4uyl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "m_gd,c_gd=0,0\n",
        "lr=0.01\n",
        "epochs=1000\n",
        "for i in range(epochs):\n",
        "  y_pred =m_gd*X + c_gd\n",
        "  error = y - y_pred\n",
        "\n",
        "  d_m  =  (-2/n)*sum(X*(y-y_pred))\n",
        "\n",
        "  d_c  =  (-2/n)*sum(y-y_pred)\n",
        "\n",
        "  m_gd -= lr*d_m\n",
        "  c_gd -= lr*d_c\n",
        "\n",
        "  df['y_pred_gd'] = m_gd * X + c_gd\n",
        "\n",
        "  mse_gd = ((df['y'] - df['y_pred_gd'])**2).mean()\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "1FRQRwD62rsg"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lasso Regression"
      ],
      "metadata": {
        "id": "YBtHx5iR7a0i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üìâ Lasso Regression Loss Function\n",
        "\n",
        "L1 Regularization\n",
        "\n",
        "$$\n",
        "{\\text{Lasso}}(\\beta) = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 + \\lambda \\sum_{j=1}^{p} |\\beta_j|\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "### üîç Where:\n",
        "\n",
        "- Lasso \\: Total cost function for Lasso Regression  \n",
        "-  n \\: number of data points\n",
        "-  yi \\: Actual value of the target variable for the \\( i^th \\) sample  \n",
        "- yi(hat) \\: Predicted value from the model for the \\( i^th \\) sample  \n",
        "- beta_j \\: Model coefficient for the \\( j^th \\) feature  \n",
        "- lambda \\: Regularization parameter (controls penalty strength)  \n",
        "-  p \\: Number of features (independent variables)  \n",
        "  \n"
      ],
      "metadata": {
        "id": "CESMcPnS7cSW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "m_lasso, c_lasso = 0, 0\n",
        "lr = 0.01\n",
        "epochs = 1000\n",
        "lmbda = 0.1\n",
        "\n",
        "for _ in range(epochs):\n",
        "    y_pred = m_lasso * X + c_lasso\n",
        "    error = y - y_pred\n",
        "\n",
        "\n",
        "    dm = (-2/n) * (X * error).sum() + lmbda * np.sign(m_lasso)\n",
        "    dc = (-2/n) * error.sum()\n",
        "\n",
        "    m_lasso -= lr * dm\n",
        "    c_lasso -= lr * dc\n",
        "\n",
        "df['y_pred_lasso'] = m_lasso * X + c_lasso"
      ],
      "metadata": {
        "id": "bp1YWPi16EiZ"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìò Ridge Regression Cost Function\n",
        "\n",
        "The cost function for Ridge Regression is:\n",
        "\n",
        "$$\n",
        "J(\\beta) = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 + \\lambda \\|\\beta\\|^2\n",
        "$$\n",
        "\n",
        "---\n",
        "Here Lambda is greater than or equal to zero for both lasso and ridge regression.\n",
        "\n"
      ],
      "metadata": {
        "id": "rP0DVCcMKa0s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "m_ridge, c_ridge = 0.0, 0.0\n",
        "lr = 0.01\n",
        "epochs = 1000\n",
        "lmbda = 0.1\n",
        "\n",
        "for _ in range(epochs):\n",
        "    y_pred = m_ridge * X + c_ridge\n",
        "    error = y - y_pred\n",
        "\n",
        "\n",
        "    dm = (-2/n) * (X * error).sum() + 2 * lmbda * m_ridge\n",
        "    dc = (-2/n) * error.sum()\n",
        "\n",
        "    m_ridge -= lr * dm\n",
        "    c_ridge -= lr * dc\n",
        "\n",
        "df['y_pred_ridge'] = m_ridge * X + c_ridge\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-USY07zIL39F"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üå≥ Decision Tree Regressor: Loss Function\n",
        "\n",
        "### üîπ Mean Squared Error (MSE):\n",
        "\n",
        "$$\n",
        "\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\bar{y})^2\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "### üìå Definitions:\n",
        "-  n : Number of data points in the node  \n",
        "- yi : Actual value for the \\( i^th )data point  \n",
        "- bar(y): Mean of all target values in the node  \n",
        "- Lower MSE ‚Üí better node split  \n"
      ],
      "metadata": {
        "id": "PgAEP3uWMKvV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mse(y):\n",
        "    return ((y - y.mean()) ** 2).mean()\n",
        "\n",
        "def best_split_regression(X, y):\n",
        "    best_mse = float('inf')\n",
        "    best_split = None\n",
        "\n",
        "    for val in sorted(X.unique()):\n",
        "        left = y[X < val]\n",
        "        right = y[X >= val]\n",
        "        if len(left) == 0 or len(right) == 0:\n",
        "            continue\n",
        "\n",
        "        score = (len(left) * mse(left) + len(right) * mse(right)) / len(y)\n",
        "\n",
        "        if score < best_mse:\n",
        "            best_mse = score\n",
        "            best_split = val\n",
        "\n",
        "    return best_split, best_mse\n",
        "\n",
        "split, loss = best_split_regression(X,y)"
      ],
      "metadata": {
        "id": "dXQJ0BQbMDTC"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üå≥ Decision Tree Classifier: Loss Functions\n",
        "\n",
        "### üîπ Gini Impurity:\n",
        "\n",
        "$$\n",
        "G = 1 - \\sum_{i=1}^{C} p_i^2\n",
        "$$\n",
        "\n",
        "### üîπ Entropy:\n",
        "\n",
        "$$\n",
        "H = -\\sum_{i=1}^{C} p_i \\log_2(p_i)\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "### üìå Definitions:\n",
        "-  C : Number of classes  \n",
        "-  p_i : Proportion of class \\( i \\) in a node  \n",
        "- Lower values of Gini or Entropy indicate purer (better) splits  \n"
      ],
      "metadata": {
        "id": "6hBz8iqDMKOP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gini(y):\n",
        "    classes = y.unique()\n",
        "    g = 1.0\n",
        "    for c in classes:\n",
        "        p = (y == c).mean()\n",
        "        g -= p ** 2\n",
        "    return g\n",
        "\n",
        "def best_split_classification(X, y):\n",
        "    best_gini = float('inf')\n",
        "    best_split = None\n",
        "\n",
        "    for val in sorted(X.unique()):\n",
        "        left = y[X < val]\n",
        "        right = y[X >= val]\n",
        "        if len(left) == 0 or len(right) == 0:\n",
        "            continue\n",
        "\n",
        "        score = (len(left) * gini(left) + len(right) * gini(right)) / len(y)\n",
        "\n",
        "        if score < best_gini:\n",
        "            best_gini = score\n",
        "            best_split = val\n",
        "\n",
        "    return best_split, best_gini\n",
        "\n",
        "split_c, loss_c = best_split_classification(X, y)\n"
      ],
      "metadata": {
        "id": "IxiWwtxKeEfS"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìä K-Means Clustering: Loss Function\n",
        "\n",
        "The objective (loss) function for K-Means clustering is:\n",
        "\n",
        "$$\n",
        "J = \\sum_{i=1}^{k} \\sum_{x \\in C_i} \\| x - \\mu_i \\|^2\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "### üìå Definitions:\n",
        "\n",
        "-  k : Number of clusters  \n",
        "- C_i : The set of data points assigned to cluster \\( i \\)  \n",
        "- mu_i : Centroid (mean) of cluster \\( i \\)  \n",
        "- x : A data point  \n",
        "- | x - mu_i |^2 : Squared Euclidean distance between the point and its cluster center  \n"
      ],
      "metadata": {
        "id": "oFg0pk9gfoHP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class KMeans:\n",
        "    def __init__(self, k_clusters, max_iters, tol=1e-4):\n",
        "        self.k_clusters = k_clusters\n",
        "        self.max_iters = max_iters\n",
        "        self.tol = tol\n",
        "\n",
        "    def fit(self, X):\n",
        "        if isinstance(X, pd.DataFrame):\n",
        "            X = X.values\n",
        "        self.X = X\n",
        "        m, n = X.shape\n",
        "        random_indices = np.random.choice(m, self.k_clusters, replace=False)\n",
        "        self.centroids = X[random_indices]\n",
        "        for i in range(self.max_iters):\n",
        "            self.labels = self.assign_clusters(X)\n",
        "            new_centroids = np.array([\n",
        "                X[self.labels == j].mean(axis=0) for j in range(self.k_clusters)\n",
        "            ])\n",
        "            if np.all(np.linalg.norm(self.centroids - new_centroids, axis=1) < self.tol):\n",
        "                break\n",
        "            self.centroids = new_centroids\n",
        "        return self\n",
        "\n",
        "    def assign_clusters(self, X):\n",
        "        distances = np.linalg.norm(X[:, np.newaxis] - self.centroids, axis=2)\n",
        "        return np.argmin(distances, axis=1)\n",
        "\n",
        "    def predict(self, X):\n",
        "        return self.assign_clusters(X)\n",
        "\n",
        "    def wcss(self):\n",
        "        return np.sum([\n",
        "            np.sum((self.X[self.labels == i] - self.centroids[i]) ** 2)\n",
        "\n",
        "            for i in range(self.k_clusters)\n",
        "        ])"
      ],
      "metadata": {
        "id": "tPk-rO_lm6mw"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eokIxpWsp2pN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}